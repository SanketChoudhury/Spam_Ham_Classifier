{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0E1r4Gi8sS9n"
   },
   "source": [
    "### Importing required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6r9_evQ1nWEp",
    "outputId": "bb89d0ab-a81a-461d-8b08-d1fb0408db79"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Importing Libraries\n",
    "\n",
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cnKX1qeyAs0"
   },
   "source": [
    "### Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "mCEkqeS9obX2",
    "outputId": "f390421f-f8e8-4bf0-9d91-a416354f8fb2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>SIX chances to win CASH! From 100 to 20,000 po...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                          statement\n",
       "0      spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "1      spam  FreeMsg Hey there darling it's been 3 week's n...\n",
       "2      spam  WINNER!! As a valued network customer you have...\n",
       "3      spam  Had your mobile 11 months or more? U R entitle...\n",
       "4      spam  SIX chances to win CASH! From 100 to 20,000 po..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dataset Loading\n",
    "dataset = pd.read_csv('spam1.csv',names=['sentiment','statement'])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4WAq2WdwvTe5"
   },
   "outputs": [],
   "source": [
    "def cleanString(incomingString):\n",
    "    newstring = incomingString\n",
    "    newstring = newstring.replace(\"!\",\"\")\n",
    "    newstring = newstring.replace(\"@\",\"\")\n",
    "    newstring = newstring.replace(\"#\",\"\")\n",
    "    newstring = newstring.replace(\"$\",\"\")\n",
    "    newstring = newstring.replace(\"%\",\"\")\n",
    "    newstring = newstring.replace(\"^\",\"\")\n",
    "    newstring = newstring.replace(\"&\",\"and\")\n",
    "    newstring = newstring.replace(\"*\",\"\")\n",
    "    newstring = newstring.replace(\"(\",\"\")\n",
    "    newstring = newstring.replace(\")\",\"\")\n",
    "    newstring = newstring.replace(\"+\",\"\")\n",
    "    newstring = newstring.replace(\"=\",\"\")\n",
    "    newstring = newstring.replace(\"?\",\" \")\n",
    "    newstring = newstring.replace(\"\\'\",\"\")\n",
    "    newstring = newstring.replace(\"\\\"\",\"\")\n",
    "    newstring = newstring.replace(\"'\",\"\")\n",
    "    newstring = newstring.replace(\"'m\",\"am\")\n",
    "    newstring = newstring.replace(\"}\",\"\")\n",
    "    newstring = newstring.replace(\"[\",\"\")\n",
    "    newstring = newstring.replace(\"]\",\"\")\n",
    "    newstring = newstring.replace(\"<\",\"\")\n",
    "    newstring = newstring.replace(\">\",\"\")\n",
    "    newstring = newstring.replace(\"~\",\"\")\n",
    "    newstring = newstring.replace(\"`\",\"\")\n",
    "    newstring = newstring.replace(\":\",\"\")\n",
    "    newstring = newstring.replace(\";\",\"\")\n",
    "    newstring = newstring.replace(\"|\",\"\")\n",
    "    newstring = newstring.replace(\"\\\\\",\"\")\n",
    "    newstring = newstring.replace(\"/\",\"\") \n",
    "    newstring = newstring.replace(\"0\",\"\")\n",
    "    newstring = newstring.replace(\"1\",\"\")\n",
    "    newstring = newstring.replace(\"2\",\"\")\n",
    "    newstring = newstring.replace(\"3\",\"\")\n",
    "    newstring = newstring.replace(\"4\",\"\")\n",
    "    newstring = newstring.replace(\"5\",\"\")\n",
    "    newstring = newstring.replace(\"6\",\"\")\n",
    "    newstring = newstring.replace(\"7\",\"\")\n",
    "    newstring = newstring.replace(\"8\",\"\")\n",
    "    newstring = newstring.replace(\"9\",\"\")  \n",
    "    newstring = newstring.replace(\".\",\"\")\n",
    "    newstring = newstring.replace(\",\",\"\")\n",
    "    return newstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "w8OVy9A7wR2l"
   },
   "outputs": [],
   "source": [
    "clean_text = []\n",
    "for _,row in dataset.iterrows():\n",
    "  text = cleanString(row[1])\n",
    "  text_tokens = word_tokenize(text)\n",
    "  tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "  text = (\" \").join(tokens_without_sw)\n",
    "  clean_text.append(text)\n",
    "dataset['statement'] = clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3Nz4xmuwY3t"
   },
   "source": [
    "### Model training and Pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1dVgreFk0Htj",
    "outputId": "e2eb0bea-089e-49f0-a684-ec4c64908dde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(264, 224)\n"
     ]
    }
   ],
   "source": [
    "#Training the model and predicting\n",
    "#Creating Vocabulary\n",
    "id = dataset['statement']\n",
    "model = Tokenizer()\n",
    "model.fit_on_texts(list(id))\n",
    "\n",
    "X = dataset['statement']\n",
    "y = dataset['sentiment']\n",
    "\n",
    "\n",
    "X = model.texts_to_matrix(X, mode='count')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "print((X_test.shape[0], (y_test == y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jqKtbJTE65Fz"
   },
   "outputs": [],
   "source": [
    "#Creating the classifier function for queries\n",
    "def classifier(text):\n",
    "  text_ = cleanString(text)\n",
    "  text_tokens = word_tokenize(text_)\n",
    "  tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "  text = (\" \").join(tokens_without_sw)\n",
    "  vec = model.texts_to_matrix([text], mode='count')\n",
    "  return gnb.predict(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LhyJP9LN7lGN",
    "outputId": "d7d2c59c-8b00-432f-ac2a-83fddeb21dbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham']\n"
     ]
    }
   ],
   "source": [
    "print(classifier(\"Sorry, I'll call later\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OvxNRf2quhbs",
    "outputId": "516d21b5-3395-4d05-c5d1-e85c9ba30a80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spam']\n"
     ]
    }
   ],
   "source": [
    "print(classifier(\"Congratulation! You won a Prize!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5kSZg5bR6q1s"
   },
   "outputs": [],
   "source": [
    "#Serialization the classifier using pickle\n",
    "pickle_out = open(\"classifier.pkl\",\"wb\")\n",
    "pickle.dump(gnb, pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_out = open(\"model.pkl\",\"wb\")\n",
    "pickle.dump(model,pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "pickle_in = open(\"classifier.pkl\",\"rb\")\n",
    "classifier=pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "sen = \"Sorry, I'll call later\"\n",
    "text_ = cleanString(sen)\n",
    "text_tokens = word_tokenize(text_)\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "text = (\" \").join(tokens_without_sw)\n",
    "pickle_in = open(\"model.pkl\",\"rb\")\n",
    "model=pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "vec = model.texts_to_matrix([text], mode='count')\n",
    "prediction = classifier.predict(vec)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Spam_classifier.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
